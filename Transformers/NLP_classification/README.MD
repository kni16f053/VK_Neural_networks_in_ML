# Домашнее задание 9. Классификация букв для задачи расстановки ударения.



**Задача:** Многоклассовая классификация — определение позиции ударения (символа `^`) в слове. Каждый класс соответствует позиции ударного слога в последовательности токенов.



## Описание проекта

Дообучение русскоязычной модели BERT для расстановки ударений в словах. Исследование влияния сильного дисбаланса классов на качество трансформерных архитектур.



## Датасет

- **Источник:** Файл `all_accents.tsv`, 36 классов после очистки (позиции 1–35, нулевой класс - отсутствие ударения)

- **Размер:** ~1.4M примеров после очистки

- **Сплит:** Train/Test = 50/50 (сохранена оригинальная разбалансировка)

- **Балансировка:** Крайне неравномерное распределение (от 288k примеров в классе 6 до единичных)



![Distribution](https://radika1.link/2026/02/12/SNIMOK-EKRANA-2026-02-12-1305373bc32cdc98e2b4bb.png)

## Ключевой инсайт



Для трансформеров **дисбаланс классов не является такой катастрофой, как для моделей, учившихся на статистических паттернах**. Self-attention позволяет модели выучить универсальные правила определения ударения через отношения между токенами, а не через частотность конкретных позиций.



## Архитектура

- **Модель:** `DeepPavlov/rubert-base-cased`

- **Голова:** SequenceClassification (36 классов)

- **Оптимизатор:** AdamW (lr=2e-5)

- **Batch size:** 500

- **Максимальная длина:** 36 токенов

- **Токенизация:** побуквенная (char-level)



## Результаты



![Results](https://radika1.link/2026/02/12/SNIMOK-EKRANA-2026-02-12-1307301b0c05ed65d86565.png)



### Основные метрики

- **Итоговая Accuracy:** 96.3% на тестовой выборке

- **Средняя Precision:** 83.0%

- **Медианная Precision:** 94.9%



### Зависимость качества от количества примеров:

- **≥250 примеров:** стабильно высокое качество (≥92%)

- **50–250 примеров:** умеренное падение (47–80%)

- **<50 примеров:** низкое качество (0–50%)

- **Единичные примеры:** модель не обобщает



### Анализ ошибок

Модель успешно выучила **лингвистически обоснованные паттерны**:

- Ударение определяется позицией слога относительно морфем, а не запоминанием слов

- Даже на редких позициях предсказания осмысленны при наличии >250 примеров

- Но единичные примеры (классы 25–30) всё так же не позволяют выявить закономерности



## Технические особенности



### Реализация

- Кастомный `WordsDataset` с побуквенной токенизацией

- Динамическое padding до `max_length=36`

- Полный pipeline: загрузка → токенизация → обучение → валидация

- Мониторинг памяти GPU и скорости обработки



### Критические решения

1. **Отказ от балансировки классов** — осознанное решение, основанное на природе self-attention

2. **Большой batch size (500)** — ускорение сходимости



## Выводы



1. **Трансформеры устойчивы к дисбалансу**, если задача имеет внутренние структурные правила

2. **Порог "достаточности" данных** для BERT в данной задаче — **≥250 примеров на класс**

3. **Механизм внимания** позволяет обобщать даже на редкие паттерны ударений, что свидетельствует о присутствии большой доли универсальных паттернов, которые модель выучила на частых классах. Они применимы и на более редкие.



## Потенциальные улучшения



- Использование DeBERTa (лучшая работа с positional embeddings)

- Добавить данные для классов с <250 примерами

- Попробовать токенизировать слоги, а не только буквы



Домашнее задание демонстрирует глубокое понимание принципов работы трансформеров и умение интерпретировать поведение модели в условиях сильного дисбаланса данных.

