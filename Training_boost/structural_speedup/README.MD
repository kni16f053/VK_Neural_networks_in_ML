# Домашнее задание 5. Методы улучшения сходимости нейронных сетей



**Задача:** Исследование и реализация методов улучшения сходимости нейронных сетей: инициализация весов (Xavier), регуляризация (DropConnect), нормализация (LayerNorm).



## Описание проекта

Проект включает сравнительный анализ различных техник ускорения и стабилизации обучения нейронных сетей на датасете MNIST. Реализованы три ключевых метода с полной интеграцией в PyTorch.



## Реализованные методы



### 1. Инициализация весов

- **Xavier Initialization:** Реализована для Tanh-активаций

- **He Initialization:** Реализована для ReLU-активаций  

- **Сравнение:** с обычной нормальной инициализацией



### 2. DropConnect

- **Реализация:** Вероятностное отключение связей (весов) вместо нейронов

- **Сравнение:** с классическим Dropout



### 3. Layer Normalization

- **Реализация:** Нормализация по измерениям для каждого объекта независимо

- **Сравнение:** с Batch Normalization



## Ключевые результаты



### Инициализация весов:

- **Xavier + Tanh:** Стабилизация дисперсий при forward/backward pass

- **He + ReLU:** Предотвращение проблемы "исчезающих/взрывающихся" градиентов

- **Наблюдение:** Сохранение дисперсии ~1.0 по всем слоям сети



### DropConnect vs Dropout:

- **DropConnect:** Отключает отдельные связи (более тонкая регуляризация)

- **Dropout:** Отключает целые нейроны (более агрессивная регуляризация)

- **Результат:** Оба метода предотвращают переобучение, DropConnect сохраняет больше информации



### LayerNorm vs BatchNorm:

- **LayerNorm:** Нормализация по нейронам для каждого объекта

- **BatchNorm:** Нормализация по батчу для каждого нейрона

- **Преимущества LayerNorm:**

&nbsp; - Работает с любым размером батча (даже с 1)

&nbsp; - Не создает зависимости между объектами в батче

&nbsp; - Более стабильная сходимость на валидации



## Технические детали

- **Архитектура:** Полносвязная сеть (784 → 500 → 100 → 100 → 10)

- **Датасет:** MNIST (28×28 рукописные цифры)

- **Активации:** Tanh (для Xavier) и ReLU (для He)

- **Фреймворк:** PyTorch

- **Метрики:** Accuracy, Loss, стабильность градиентов



## Основные выводы

1. **Правильная инициализация** критически важна для глубоких сетей

2. **Xavier/He** обеспечивают сохранение дисперсии градиентов по слоям

3. **DropConnect** — эффективная альтернатива Dropout для тонкой регуляризации  

4. **LayerNorm** превосходит BatchNorm в задачах с маленькими батчами

5. **Комбинация методов** дает синергетический эффект для стабильного обучения



## Структура кода

- `NetworkInitializer()` — фабрика сетей с разными инициализациями

- Реализация `DropConnect` как пользовательского слоя PyTorch

- Реализация `LayerNorm` с нуля

- Полный цикл обучения с визуализацией



Домашнее задание демонстрирует глубокое понимание внутренней работы нейронных сетей и методов их оптимизации.

