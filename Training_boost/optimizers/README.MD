\# Домашнее задание 4. Сравнение методов оптимизации для нейронных сетей



\*\*Задача:\*\* Реализация и сравнительный анализ методов оптимизации (Momentum, AdaDelta, Adam) для обучения нейронных сетей на датасете MNIST.



\## Описание проекта

Проект включает полную реализацию 7-ми методов оптимизации с нуля и их тестирование на двух задачах:

1\. \*\*Визуализация сходимости\*\* на синтетической функции потерь

2\. \*\*Обучение нейронных сетей\*\* на MNIST (полносвязная и сверточная архитектуры)



\## Реализованные методы оптимизации

1\. \*\*SGD\*\* (Stochastic Gradient Descent)

2\. \*\*Momentum\*\* (с накоплением импульса)

3\. \*\*NAG\*\* (Nesterov Accelerated Gradient)

4\. \*\*Adagrad\*\* (Adaptive Gradient)

5\. \*\*RMSProp\*\* (Root Mean Square Propagation)

6\. \*\*AdaDelta\*\* (с адаптивным learning rate)

7\. \*\*Adam\*\* (Adaptive Moment Estimation)



Все методы реализованы как классы, наследуемые от базового `Optimizer` с методами `step()`, `zero\_grad()`.



\## Эксперименты и результаты



\### 1. Визуализация на синтетической задаче

\- Функция потерь: MSE для линейной регрессии (y = 3x + 2 + noise)

\- \*\*Лучший метод:\*\* Adam (быстрейшая сходимость)

\- \*\*Наблюдение:\*\* AdaDelta чувствителен к начальной инициализации



\### 2. Обучение полносвязной сети на MNIST

\- Архитектура: 784 → 100 → 100 → 10 (с ReLU)

\- \*\*Лучшие методы:\*\* SGD, Adagrad, Adam (сопоставимые результаты ~98% accuracy)

\- \*\*Худшие:\*\* Momentum, NAG (отсутствие адаптивного learning rate)



\### 3. Обучение сверточной сети на MNIST

\- Архитектура: 2 слоя Conv2d + MaxPool + полносвязные слои

\- \*\*Лучший метод:\*\* Adam (~99% accuracy на тестовой выборке)

\- \*\*Наблюдение:\*\* CNN обучается лучше MLP для задачи распознавания цифр



\## Ключевые выводы

1\. \*\*Adam\*\* демонстрирует стабильно хорошие результаты благодаря комбинации momentum и адаптивного learning rate.

2\. \*\*Адаптивные методы\*\* (Adam, RMSProp) лучше справляются с неоднородными ландшафтами функций потерь.

3\. \*\*Классические методы\*\* (SGD, Momentum) требуют тщательного подбора learning rate.

4\. \*\*AdaDelta\*\* чувствителен к инициализации, но может показывать хорошие результаты при правильной настройке.



\## Технические детали

\- \*\*Стек:\*\* PyTorch, NumPy, Matplotlib

\- \*\*Визуализация:\*\* 3D-анимация сходимости методов оптимизации

\- \*\*Датасет:\*\* MNIST (60k тренировочных, 10k тестовых изображений)

\- \*\*Метрики:\*\* Accuracy, Loss (CrossEntropy)



\## Структура кода

\- `Optimizer` — базовый класс для всех методов

\- `SGD`, `Momentum`, `NAG`, `Adagrad`, `RMSProp`, `AdaDelta`, `Adam` — реализации методов

\- `LossAnimator` — класс для визуализации сходимости

\- `MLP\_MNIST`, `CNN\_MNIST` — архитектуры нейронных сетей

\- Полный цикл обучения с валидацией



Домашнее задание демонстрирует глубокое понимание принципов работы оптимизаторов и их практическое применение в задачах машинного обучения.

