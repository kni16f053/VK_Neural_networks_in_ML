Содержимое папки ./MLP_with_numpy/table_proc/
- HW_2.ipynb
- README.MD

# Домашнее задание 2. Классификация рукописных цифр: исследование архитектур и аугментаций

**Задача:** Классификация изображений рукописных цифр из датасета MNIST с исследованием влияния функций активации и методов аугментации данных на качество модели.

## Описание проекта
Полносвязная нейронная сеть реализована с нуля на NumPy (без использования готовых фреймворков типа PyTorch/TensorFlow). Проект включает:
- Реализацию базовых слоёв (Linear, Sigmoid, ReLU, ELU, Tanh) и loss-функции (NLLLoss)
- Сравнение различных функций активации
- Исследование методов аугментации данных для улучшения обобщающей способности

## Архитектура модели
```
784 → Linear → Activation → 100 → Linear → Activation → 100 → Linear → Softmax → 10
```
где Activation ∈ {Sigmoid, ReLU, ELU, Tanh}

## Реализованные методы аугментации
1. **Повороты:** ±15 градусов
2. **Случайные сдвиги:** до 3 пикселей
3. **Добавление шума:** Gaussian noise
4. **Mixup:** линейная интерполяция изображений

## Ключевые результаты
### Сравнение функций активации (accuracy на тестовой выборке):
- **ReLU/ELU:** ~97.5% (наилучший результат)
- **Tanh:** ~96.8%
- **Sigmoid:** ~95.2% (наихудшая сходимость)

### Эффективность аугментаций:
- **Повороты (+Rotate):** +0.8% к accuracy
- **Все аугментации вместе:** +1.2% к accuracy
- **Наибольший прирост:** комбинация поворотов и шума
- **Компромисс:** увеличение времени обучения в 2-3 раза

## Технические детали
- **Стек:** Python, NumPy, PyTorch (только для загрузки данных), scikit-image
- **Обучение:** 20 эпох, batch_size=32, learning_rate=0.01
- **Метрики:** Accuracy, Negative Log-Likelihood Loss
- **Визуализация:** анализ ошибок, графики сходимости

## Основные выводы
1. **ReLU/ELU** показывают лучшую сходимость для задачи классификации изображений
2. **Аугментации** существенно улучшают обобщающую способность, особенно на искажённых данных
3. **Повороты** — наиболее эффективный метод аугментации для MNIST
4. **Реализация с нуля** демонстрирует глубокое понимание работы нейронных сетей

## Структура кода
- `Linear`, `Sigmoid`, `ReLU`, `ELU`, `Tanh` — слои нейронной сети
- `NeuralNetwork` — контейнер для последовательности слоёв
- `NLLLoss` — функция потерь
- Полный цикл обучения с визуализацией

Домашнее задание демонстрирует фундаментальные навыки в машинном обучении: от реализации базовых алгоритмов до экспериментального исследования методов улучшения моделей.
```
