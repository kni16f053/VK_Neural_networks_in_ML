Содержимое папки ./MLP_with_numpy/table_proc/
- dataset.txt
- HW_1.ipynb
- README.MD

# Домашнее задание 1. Реализация Gradient Descent с нуля.

**Задача:** Прогнозирование стоимости бриллиантов на основе их характеристик с использованием собственных реализаций алгоритмов градиентного спуска.

## Описание данных
Датасет `diamonds.csv` содержит характеристики бриллиантов: вес (`carat`), качество огранки (`cut`), цвет (`color`), чистоту (`clarity`), размеры (`x`, `y`, `z`), глубину (`depth`), ширину площадки (`table`) и целевую переменную — цену в долларах (`price`).

## Что было реализовано

### 1. Линейная регрессия с градиентным спуском
- Реализован класс `GDLinearRegressor` с нуля на NumPy
- Оптимизация методом градиентного спуска
- Сравнение с `LinearRegression` из scikit-learn

### 2. Многослойный перцептрон (MLP) с обратным распространением ошибки
- Полностью векторизованная реализация на NumPy
- Поддержка различных функций активации: Sigmoid, Tanh, ReLU
- Архитектура с тремя скрытыми слоями (20, 30, 40 нейронов)
- Реализация backpropagation для задачи регрессии

## Результаты
- **Линейная регрессия (своя реализация):** Сопоставимое качество с sklearn-версией
- **MLP с ReLU:** Наилучшие результаты — гистограмма предсказаний близка к распределению реальных цен
- **Ключевое наблюдение:** ReLU показала лучшую сходимость для задачи регрессии, устранив проблему отрицательных предсказаний

## Технические детали
- **Стек:** Python, NumPy, pandas, scikit-learn, matplotlib
- **Препроцессинг:** One-hot encoding категориальных признаков, нормализация MinMaxScaler
- **Метрика:** MSE (Mean Squared Error)
- **Разделение:** 70/30 (train/test)

## Вывод
Проект демонстрирует глубокое понимание внутренней работы базовых алгоритмов машинного обучения через их реализацию с нуля. Полученные реализации корректно обучаются и показывают результаты, сопоставимые с библиотечными аналогами.